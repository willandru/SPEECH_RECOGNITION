-Automatic Speech REcognition

- Audio: .mp3?---> .wav  :  wav2vec (una seña .wav convertirla a un vector/lista de numeros)
							wav2text ( una señal .wav convertirla a una cadena de texto, con las palabras pronunciadas)
							text2wav (un texto convertirlo en una señal de audio .wav)

- Machine LEarning: Aprendizaje Supervisado, Aprendizaje No Supervisado, Aprendizaje por Refuerzo,.

- FEATURES("/fi-tchurs/")/ CARACTERISTICAS: Las features o caracteristicas de los datos, se refiere , en terminos generales, a las "propiedades" mas significativas o descriptivas o discriminativas de una medicion de algun dato que se tenga. Es decir, en el caso de el AUDIO(.wav), las "features" son todo el conjunto de posibles caracteristicas del AUDIO. La misma esñal es una feature, pero tambien los es su transformada de fourier, su Espectrograma , Su duracion, su frecuencia de muestreo.. entre otras, es decir los datos que permiten caracterizar a cada uno de los datos.. En una tabla de datos , esto se vería como cada columna representa una feature de una fila en especifico, cada fila corresponde a una medicion realizada. 
-FONEMAS: Los ladrillos de "sonido" fundamentales de una letra u vocale, que son caracteristicas de un lenguaje. 

-TRANSFORMACIONES: Podemos hacer operaciones o descompocisiones, camvbios de espacios vectoriles, entre muchas cosas matematicas, que reciben como entrada un dato y lo mapean de forma UNICA a otro subespacio matematico.
Por ejemplo, la transfomrada de foruriier es una funcion "wav2fun", la cual recibe una señal dependiente del tiempo y devuelve una señal dependiendte de la frecuencia.
- Espectrograma: Una grafica donde x es el tiempo, frecuencias en el eje y , y la intensidad de los colores representa la intensidad del sonido. Se obtiene a partir de NFTT
- Transformada de Fourier: 

- Mel Espectogram

- Feature Extaction Methods:
		- Perceptual Linear Prediction (PLP)
		- Relative espectra filtering of long domain coefficients PLP (RASTA-PLP)
		- Linear Predictive COding (LPC)
		- Predictive Cepstral COefficient 
		- Mel scale cepstral analysis (MEL)
		- Mel-fecuency cepstral coefficients (MFCC)
		outdated : Power spectral analysis (FFT), first order derivative (DELTA), Energy normalization.

- Cadenas de Markov: Es un modelo matematico MUY importante. Los estados y las transcisiones de probabilidad de un estado a otro. Permite controlar un agente en el APRENDIZAJE POR REFUERZO, es decir el modelo de Markov se utiliza para hacer aprendizaje por refuerzo, tambien se utiliza para modelar los problemas del mundo a traves de estados. EN este caso, he leido mucho que mencionan este tema para hacer Reconocimiento del Habla.

- Modelo OCULTO DE MARKOV: el modelo que se utiliza para speech recognition, reconocimiento de fonemas

- Redes Nueronales de Markov: Son arquitecturas diseñadas para modelar los problemas de markov.

-Maquinas de estado FInito

- Modelo de Mezcla Gaussiana (Gaussian Mixture MOdel): Tampoco se aun muy bien que es esto la verdad, pero se que es muuuuy util.

-Teoria de Speech RECOGNITION-- desde 1950 o antes .
-fonetos, silabas, ......., descompocicion del audio.
- Algoritmos de alineamiento en 1 paso, en 2 pasos: Algoritmos que permiten alinear una señal de audio con una base de señales para reconocer las palabras que estan presentes en la señal de entrada. Permite hacer reconocimiento de 1 palabra o de una secuencia de palabras.

-Deep Learning: Redes Neuronales con muchas neuronas ocultas, muchas capas con muchas neuronas cada capa, entre mas capas y neuronas más "profundidad" tiene la red neuronal, asi mismo mayor el costo coputacional.

-N-gramas (aun no se que es, pero es algo con nodos que se conectan entre si, asi como en MARKOV, O BUENO ESO CREO, la cosa es que estn por ahi en los libros y en los videos, valen la pena investigar)

- REdes Neuronales Recurrentes (RNR): Es una arquitectura especializada en el analisis de datos secuenciales como una señal que depende del tiempo. Es idieal para analizar audio, sin embargo a diferencia de los transformers, esta arquitectura analia cada dato neurona por neurona, es decir hasta que una neurona no haya procesado la informacion, el flujo no continuara a la siguiente neurona. Este rpoblema hace que el entrenamiento de los datos sea muy costoso en cuanto a tiempo y memoria. Es por esto que la arquitectura más nueva transformer es tan llamativa. Ademas una RNR pura no es tan buena para recordar la importancia de datos que hayan ocurrido muy en el pasado, pero que aun influyan ene el futuro para hacer las predicciones.

-Redes Neuronales LSTM: Long-Short-Term-Memory Neurons. Es una arquitectura de red neuronal derivado de las RNR. Permite tener "memoria" de las cosas importantes ("atencion") , en DATOS SECUENCIALES .

- Redes Neuronales Transformers: Una red neuronal que involucra la capacidad de "atencion", es decir, de todos los datos de entrada de manera secuencial, la red aprende a reconocer que datos valen más la pena que otros y les presta mayor "atencion" a estos. 

- Redes Neuronales Generativas, Autoencoders, Degenerativas o Adversarias: Un tipo de REd Neuronal muy interesante/importante, permite genear datos nuevos, muy reales, osea sintetizar nuevas voces, nuevas palabras, nuevas frases, lo que sea, permite generar datos nuevos. Cuando se tienen pocos datos (en nuestro caso), permiten hacer "data augmentation" o amplificar un dataset de 100 datos a 500 más, que son todos diferentes pero que pertenecen a la misma distribucion de probabilidad de los 100 datos originales. 

